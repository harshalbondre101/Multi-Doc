Think of yourself as a python developer. You are specialised in concise,clean, maintainable code writing .
You have experience with various machine learning algorithm and rule base logic building you will be provided with driscptive outline of an application your tasks is to provide the code for same write the program in python use library such as strelmlit ,skleran ,pandas, and a file handling library and other suitable library 
Do not provide hole code provide one by one module

Program discription: 
Write a program useing streamlit interface for multiple documents summarision the input can be a single text file or any other document type such as pdf docx (include all text file types) this program should include further module
1. A data cleaning module which take the text input text (main program with streamlit interface)
2. A module that calculate cosine similarity and remove similar part from the text 
3. A module that cluster the data and take single data point(best one) and remove remaining elements from the cluster
4. A module that models topic around text. i.e. topic modelling. it shoud provide diffrents topics for each file or document. 
5. A module that uses TF-IDF to identify important parts and remove least important part.
6. A module for extractive text summarization of given input.

Program flow:
The main program is a streamlit interface. where user can upload any number of document. file type can be anything. So it should dynamically load the data and create consistant documents from each which can be handled easily. 
Then the documents are cleaned using a cleaning module. 
Now here we will use a rule base approch to do the following according to the size of data.
1) Cosine similarity
2) Clustering
3) Topic modelling
4) TF_IDF 

if large ammount of data is present then. it should remove high percentage of similer text using cosine similarity. Form less number of clusters and removal of cluster element while keeping the best datapoints. the range of clusters can be 200 to 10000. less number of clusters for large size of data. Also it should model topics arround the documents and provide insights about the data. Lastly It should calculate the TFIDF and keep only important part of the data. 

else if the data is less. all the things should adjust accordingly. 
Now you need to provide the logic yourself for this rule engine. 

Lastly it should call the summarization function from summarization module and display the summary of the data.

Now try to make the interface more informative and good looking. provide each module only by one. add implicit and explicit content accordingly. 
ChatGPT
Certainly, breaking this down into modular components will help organize the application. Let's start with the data cleaning module. For this, the Streamlit interface will be developed first, allowing the user to upload documents. Once documents are uploaded, the data cleaning module will preprocess the text.

Here's a basic skeleton of how the Streamlit interface and the data cleaning module could be structured:

Streamlit Interface Module:
This module establishes the Streamlit interface, allowing users to upload their documents.

JPT
Run
Edit
Copy
Upload
Download
import streamlit as st
from data_cleaning_module import clean_data

# Function to upload and process documents
def main():
    st.title("Document Summarization App")
    uploaded_files = st.file_uploader("Upload Documents", type=['txt', 'pdf', 'docx'], accept_multiple_files=True)

    if st.button("Summarize"):
        if uploaded_files:
            cleaned_data = clean_data(uploaded_files)
            # Further processing and modules to be integrated here

# Run the app
if __name__ == '__main__':
    main()
Data Cleaning Module:
This module cleans the uploaded text data, prepping it for further processing.

JPT
Run
Edit
Copy
Upload
Download
from io import StringIO
import pandas as pd

# Function to clean data
def clean_data(uploaded_files):
    data = []

    for file in uploaded_files:
        file_text = file.getvalue().decode("utf-8")
        # Perform basic text preprocessing - tokenization, lowercasing, removing special characters, etc.
        # Add text to data list after cleaning

    return data
This is the initial part. The data cleaning module would typically involve several text preprocessing steps like tokenization, stop-word removal, and possibly lemmatization or stemming depending on the requirements.

Should I proceed with the other modules like cosine similarity, clustering, topic modeling, TF-IDF, and summarization?