import streamlit as st
from data_cleaning_module import clean_data
from cosine_similarity_module import calculate_cosine_similarity
from clustering_module import cluster_data
from topic_modeling_module import model_topics
from tfidf_module import identify_important_parts
from summarization_module import generate_summary, abstractive_summarization
import os
import textract
import pdfplumber
import io
import tempfile
from docx import Document

# Function to read text from PDF, DOC, and TXT files
def read_text(file_path):
    if file_path.endswith(".pdf"):
        with pdfplumber.open(file_path) as pdf:
            text = ""
            for page in pdf.pages:
                text += page.extract_text()
        return text
    elif file_path.endswith((".doc", ".docx")):
        text = textract.process(file_path).decode("utf-8")
        return text
    elif file_path.endswith(".txt"):
        with open(file_path, "r") as file:
            text = file.read()
        return text
    

# Function to determine file type
def get_file_extension(file_name):
    _, extension = os.path.splitext(file_name)
    return extension

# Function to dynamically calculate threshold
def calculate_threshold(data):
    total_docs = len(data)
    total_text_length = sum(len(doc) for doc in data)

    # Adjust thresholds based on your specific criteria
    if total_docs > 10 or total_text_length > 10000:
        threshold = 10  # Advanced processing
    else:
        threshold = 5  # Basic processing

    return threshold

# Function to read text from DOCX files
def read_docx(file_path):
    doc = Document(file_path)
    full_text = []
    for para in doc.paragraphs:
        full_text.append(para.text)
    return '\n'.join(full_text)

# Function to upload and process documents
def main():
    st.title("Document Summarization App")

    uploaded_files = st.file_uploader("Upload your files", type=["pdf", "doc", "docx", "txt"], accept_multiple_files=True)
    texts = []
    if uploaded_files:
        for file in uploaded_files:
            file_content = file.read()
            file_extension = os.path.splitext(file.name)[1]

            if file_extension == ".pdf":
                with pdfplumber.open(io.BytesIO(file_content)) as pdf:
                    text = ""
                    for page in pdf.pages:
                        text += page.extract_text()
            elif file_extension == ".docx":
                text = read_docx(io.BytesIO(file_content))  # Using python-docx for .docx files
            elif file_extension == ".doc":
                text = read_docx(file)  # Using python-docx for .doc files
            else:  # For .txt files
                text = file_content.decode("utf-8")
            
            texts.append(text)


    if st.button("Summarize"):
        if uploaded_files:
            #---------------------CLEANING----------------#
            st.write("Cleaning data...")
            cleaned_data = clean_data(texts)

            #---------------------COSINE SIMILARITY----------------#            

            st.write("Calculating cosine similarity...")
            cosine_data = calculate_cosine_similarity(cleaned_data)
            #st.write(cosine_data)

            #---------------------TFIDF SIMILARITY----------------#

            st.write("Identifying important parts...")
            important_parts = identify_important_parts(cosine_data)
            #st.write(important_parts)

            #---------------------TOPIC MODELLING----------------#

            st.write("Modeling topics...")
            topic_insights = model_topics(important_parts)

            # Display the topics generated by LDA
            st.write("Topic Insights:")
            for topic in topic_insights:
                st.write(topic)


            #---------------------CLUSTERING ----------------#
            st.write("Clustering data...")
            clustered_data = cluster_data(important_parts)
            #st.write(clustered_data)
            
            #---------------------SUMMARY ----------------#
            st.write("Generating summary...")
            summary = abstractive_summarization(clustered_data)

            # Display the summary
            st.write("Summary:")
            st.write(summary)

# Run the app
if __name__ == '__main__':
    main()


"""
def main():
    st.title("Document Summarization App")

    uploaded_files = st.file_uploader("Upload your files", type=["pdf", "doc", "docx", "txt"], accept_multiple_files=True)
    texts = []
    if uploaded_files:
        for file in uploaded_files:
            file_content = file.read()
            file_extension = os.path.splitext(file.name)[1]

            if file_extension == ".pdf":
                with pdfplumber.open(io.BytesIO(file_content)) as pdf:
                    text = ""
                    for page in pdf.pages:
                        text += page.extract_text()
            elif file_extension == ".docx":
                text = read_docx(io.BytesIO(file_content))  # Using python-docx for .docx files
            elif file_extension == ".doc":
                text = read_docx(file)  # Using python-docx for .doc files
            else:  # For .txt files
                text = file_content.decode("utf-8")
            
            texts.append(text)

    if st.button("Summarize"):
        if uploaded_files:
            #---------------------CLEANING----------------#
            #st.write("Cleaning data...")
            cleaned_data = clean_data(texts)
           # st.write(cleaned_data)

            #---------------------COSINE SIMILARITY----------------#            

            #st.write("Calculating cosine similarity...")
            cosine_data = calculate_cosine_similarity(cleaned_data)
            #st.write(cosine_data)

                        #---------------------TFIDF SIMILARITY----------------#

            #st.write("Identifying important parts...")
            important_parts = identify_important_parts(cosine_data)
            #st.write(important_parts)

                        #---------------------TOPIC MODELLING----------------#

            #st.write("Modeling topics...")
            topic_insights = model_topics(important_parts)

            # Display the topics generated by LDA
            #st.write("Topic Insights:")
            #for topic in topic_insights:
                #st.write(topic)


                        #---------------------CLUSTERING ----------------#
            #st.write("Clustering data...")
            clustered_data = cluster_data(important_parts)
            #st.write(clustered_data)

                        #---------------------SUMMARY ----------------#
            st.write("Generating summary...")
            summary = summarize_text_with_openai(clustered_data)

                        # Display the summary
            st.write("Summary:")
            st.write(summary)
"""
